{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RQ1: AI framing analysis\n",
    "\n",
    "This notebook analyses how artificial intelligence is framed in Swedish parliamentary documents, comparing bottom-up proposals (motioner) with top-down government bills (propositioner). We examine whether AI is presented primarily as an opportunity, a risk, or both."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup and imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from pathlib import Path\n",
    "from collections import defaultdict, Counter\n",
    "from datetime import datetime\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from bs4 import BeautifulSoup\n",
    "from nltk.sentiment import SentimentIntensityAnalyzer\n",
    "\n",
    "# configure visualisation defaults\n",
    "plt.rcParams['font.family'] = 'monospace'\n",
    "plt.rcParams['font.size'] = 10\n",
    "sns.set_palette(['#9b59b6', '#e91e63', '#34495e', '#95a5a6'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data loading\n",
    "\n",
    "Load all documents from both directories and parse their metadata headers to create a structured dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_document(filepath):\n",
    "    \"\"\"extract metadata and content from a parliamentary document file.\"\"\"\n",
    "    with open(filepath, 'r', encoding='utf-8') as f:\n",
    "        content = f.read()\n",
    "    \n",
    "    # split header from content\n",
    "    parts = content.split('=' * 80)\n",
    "    if len(parts) < 2:\n",
    "        return None\n",
    "    \n",
    "    header = parts[0]\n",
    "    doc_content = parts[1].strip()\n",
    "    \n",
    "    # extract metadata fields\n",
    "    metadata = {}\n",
    "    for line in header.strip().split('\\n'):\n",
    "        if ':' in line:\n",
    "            key, value = line.split(':', 1)\n",
    "            metadata[key.strip().lower()] = value.strip()\n",
    "    \n",
    "    return {\n",
    "        'doc_id': metadata.get('document id', ''),\n",
    "        'title': metadata.get('title', ''),\n",
    "        'date': metadata.get('date', ''),\n",
    "        'year': metadata.get('parliamentary year', ''),\n",
    "        'search_term': metadata.get('search term', ''),\n",
    "        'content': doc_content,\n",
    "        'filepath': str(filepath)\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load motioner\n",
    "mot_dir = Path('/Users/simon/Dropbox/wrk/active/research/riksdagen-data/swe-rd-ai/data/motioner')\n",
    "mot_files = list(mot_dir.glob('*.txt'))\n",
    "mot_docs = [parse_document(f) for f in mot_files]\n",
    "mot_docs = [doc for doc in mot_docs if doc is not None]\n",
    "\n",
    "for doc in mot_docs:\n",
    "    doc['doc_type'] = 'mot'\n",
    "\n",
    "# load propositioner\n",
    "prop_dir = Path('/Users/simon/Dropbox/wrk/active/research/riksdagen-data/swe-rd-ai/data/propositioner')\n",
    "prop_files = list(prop_dir.glob('*.txt'))\n",
    "prop_docs = [parse_document(f) for f in prop_files]\n",
    "prop_docs = [doc for doc in prop_docs if doc is not None]\n",
    "\n",
    "for doc in prop_docs:\n",
    "    doc['doc_type'] = 'prop'\n",
    "\n",
    "# combine into dataframe\n",
    "all_docs = mot_docs + prop_docs\n",
    "df = pd.DataFrame(all_docs)\n",
    "\n",
    "print(f\"loaded {len(mot_docs)} motioner and {len(prop_docs)} propositioner\")\n",
    "print(f\"total documents: {len(df)}\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text preprocessing\n",
    "\n",
    "Remove HTML tags and extract clean text for analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_html(html_text):\n",
    "    \"\"\"remove html tags and extract plain text.\"\"\"\n",
    "    soup = BeautifulSoup(html_text, 'html.parser')\n",
    "    text = soup.get_text()\n",
    "    # normalise whitespace\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "    return text\n",
    "\n",
    "df['clean_text'] = df['content'].apply(clean_html)\n",
    "df['word_count'] = df['clean_text'].apply(lambda x: len(x.split()))\n",
    "\n",
    "print(f\"average document length: {df['word_count'].mean():.0f} words\")\n",
    "df.groupby('doc_type')['word_count'].describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dictionary-based framing analysis\n",
    "\n",
    "Define Swedish word lists representing opportunity and risk frames, then count occurrences in each document."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# opportunity framing keywords\n",
    "opportunity_words = [\n",
    "    'möjlighet', 'potential', 'tillväxt', 'innovation', 'effektivitet',\n",
    "    'konkurrenskraft', 'utveckling', 'framsteg', 'nytta', 'fördel',\n",
    "    'förbättra', 'förbättring', 'modernisera', 'modernisering',\n",
    "    'optimera', 'optimering', 'produktivitet', 'lösning', 'framtid',\n",
    "    'tillgång', 'styrka', 'resurs', 'investering', 'satsning'\n",
    "]\n",
    "\n",
    "# risk framing keywords\n",
    "risk_words = [\n",
    "    'hot', 'risk', 'fara', 'arbetslöshet', 'integritet', 'övervakning',\n",
    "    'ojämlikhet', 'etik', 'etisk', 'oro', 'utmaning', 'problem',\n",
    "    'sårbarhet', 'sårbara', 'motverka', 'begränsa', 'begränsning',\n",
    "    'skydda', 'skydd', 'kontroll', 'reglera', 'reglering',\n",
    "    'missbruk', 'diskriminering', 'fördom', 'bias'\n",
    "]\n",
    "\n",
    "print(f\"opportunity dictionary: {len(opportunity_words)} words\")\n",
    "print(f\"risk dictionary: {len(risk_words)} words\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_framing_words(text, word_list):\n",
    "    \"\"\"count occurrences of framing words in text.\"\"\"\n",
    "    text_lower = text.lower()\n",
    "    count = 0\n",
    "    for word in word_list:\n",
    "        # word boundary matching\n",
    "        count += len(re.findall(r'\\b' + word + r'\\w*\\b', text_lower))\n",
    "    return count\n",
    "\n",
    "# calculate framing scores\n",
    "df['opp_count'] = df['clean_text'].apply(lambda x: count_framing_words(x, opportunity_words))\n",
    "df['risk_count'] = df['clean_text'].apply(lambda x: count_framing_words(x, risk_words))\n",
    "\n",
    "# normalise by document length (per 1000 words)\n",
    "df['opp_per_1k'] = (df['opp_count'] / df['word_count']) * 1000\n",
    "df['risk_per_1k'] = (df['risk_count'] / df['word_count']) * 1000\n",
    "\n",
    "# calculate framing ratio (opp - risk, normalised)\n",
    "df['framing_ratio'] = df['opp_per_1k'] - df['risk_per_1k']\n",
    "\n",
    "df[['doc_id', 'doc_type', 'opp_count', 'risk_count', 'opp_per_1k', 'risk_per_1k', 'framing_ratio']].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Framing comparison by document type\n",
    "\n",
    "Compare the distribution of opportunity and risk framing between motioner and propositioner."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# summary statistics by document type\n",
    "framing_summary = df.groupby('doc_type').agg({\n",
    "    'opp_per_1k': ['mean', 'median', 'std'],\n",
    "    'risk_per_1k': ['mean', 'median', 'std'],\n",
    "    'framing_ratio': ['mean', 'median', 'std']\n",
    "}).round(2)\n",
    "\n",
    "framing_summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualisation: opportunity vs risk frequencies by document type\n",
    "fig, axes = plt.subplots(1, 2, figsize=(12, 5))\n",
    "\n",
    "# absolute counts\n",
    "doc_types = ['mot', 'prop']\n",
    "opp_means = [df[df['doc_type'] == dt]['opp_per_1k'].mean() for dt in doc_types]\n",
    "risk_means = [df[df['doc_type'] == dt]['risk_per_1k'].mean() for dt in doc_types]\n",
    "\n",
    "x = np.arange(len(doc_types))\n",
    "width = 0.35\n",
    "\n",
    "axes[0].bar(x - width/2, opp_means, width, label='opportunity', color='#9b59b6')\n",
    "axes[0].bar(x + width/2, risk_means, width, label='risk', color='#e91e63')\n",
    "axes[0].set_xlabel('document type')\n",
    "axes[0].set_ylabel('mean frequency (per 1000 words)')\n",
    "axes[0].set_title('opportunity vs risk framing by document type')\n",
    "axes[0].set_xticks(x)\n",
    "axes[0].set_xticklabels(['motioner', 'propositioner'])\n",
    "axes[0].legend()\n",
    "axes[0].grid(axis='y', alpha=0.3)\n",
    "\n",
    "# framing ratio distribution\n",
    "axes[1].boxplot(\n",
    "    [df[df['doc_type'] == 'mot']['framing_ratio'], \n",
    "     df[df['doc_type'] == 'prop']['framing_ratio']],\n",
    "    labels=['motioner', 'propositioner'],\n",
    "    patch_artist=True,\n",
    "    boxprops=dict(facecolor='#9b59b6', alpha=0.6),\n",
    "    medianprops=dict(color='#34495e', linewidth=2)\n",
    ")\n",
    "axes[1].axhline(0, color='#95a5a6', linestyle='--', linewidth=1, alpha=0.7)\n",
    "axes[1].set_ylabel('framing ratio (opp - risk per 1k words)')\n",
    "axes[1].set_title('distribution of framing ratios')\n",
    "axes[1].grid(axis='y', alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Context window analysis\n",
    "\n",
    "Extract text surrounding AI mentions and analyse sentiment in these contexts to understand how AI is discussed when explicitly referenced."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ai search terms (case insensitive)\n",
    "ai_patterns = [\n",
    "    r'\\bai\\b',\n",
    "    r'artificiell intelligens',\n",
    "    r'artificiella intelligens'\n",
    "]\n",
    "\n",
    "def extract_ai_contexts(text, window=50):\n",
    "    \"\"\"extract text windows around ai mentions.\"\"\"\n",
    "    contexts = []\n",
    "    text_lower = text.lower()\n",
    "    \n",
    "    for pattern in ai_patterns:\n",
    "        for match in re.finditer(pattern, text_lower):\n",
    "            start = max(0, match.start() - window)\n",
    "            end = min(len(text), match.end() + window)\n",
    "            context = text[start:end]\n",
    "            contexts.append(context)\n",
    "    \n",
    "    return contexts\n",
    "\n",
    "df['ai_contexts'] = df['clean_text'].apply(extract_ai_contexts)\n",
    "df['ai_mention_count'] = df['ai_contexts'].apply(len)\n",
    "\n",
    "print(f\"total ai mentions: {df['ai_mention_count'].sum()}\")\n",
    "df.groupby('doc_type')['ai_mention_count'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sentiment analysis on context windows\n",
    "# note: vader is trained on english; results should be interpreted cautiously\n",
    "sia = SentimentIntensityAnalyzer()\n",
    "\n",
    "def analyse_context_sentiment(contexts):\n",
    "    \"\"\"calculate average sentiment across all context windows.\"\"\"\n",
    "    if not contexts:\n",
    "        return {'pos': 0, 'neg': 0, 'neu': 0, 'compound': 0}\n",
    "    \n",
    "    scores = [sia.polarity_scores(ctx) for ctx in contexts]\n",
    "    avg_scores = {\n",
    "        'pos': np.mean([s['pos'] for s in scores]),\n",
    "        'neg': np.mean([s['neg'] for s in scores]),\n",
    "        'neu': np.mean([s['neu'] for s in scores]),\n",
    "        'compound': np.mean([s['compound'] for s in scores])\n",
    "    }\n",
    "    return avg_scores\n",
    "\n",
    "df['context_sentiment'] = df['ai_contexts'].apply(analyse_context_sentiment)\n",
    "df['sentiment_compound'] = df['context_sentiment'].apply(lambda x: x['compound'])\n",
    "df['sentiment_pos'] = df['context_sentiment'].apply(lambda x: x['pos'])\n",
    "df['sentiment_neg'] = df['context_sentiment'].apply(lambda x: x['neg'])\n",
    "\n",
    "# filter documents with ai mentions for meaningful sentiment analysis\n",
    "df_with_ai = df[df['ai_mention_count'] > 0]\n",
    "\n",
    "print(f\"documents with ai mentions: {len(df_with_ai)}\")\n",
    "df_with_ai.groupby('doc_type')[['sentiment_compound', 'sentiment_pos', 'sentiment_neg']].mean().round(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualisation: context sentiment by document type\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "\n",
    "ax.boxplot(\n",
    "    [df_with_ai[df_with_ai['doc_type'] == 'mot']['sentiment_compound'],\n",
    "     df_with_ai[df_with_ai['doc_type'] == 'prop']['sentiment_compound']],\n",
    "    labels=['motioner', 'propositioner'],\n",
    "    patch_artist=True,\n",
    "    boxprops=dict(facecolor='#9b59b6', alpha=0.6),\n",
    "    medianprops=dict(color='#34495e', linewidth=2)\n",
    ")\n",
    "\n",
    "ax.axhline(0, color='#95a5a6', linestyle='--', linewidth=1, alpha=0.7)\n",
    "ax.set_ylabel('vader compound sentiment score')\n",
    "ax.set_title('sentiment around ai mentions by document type')\n",
    "ax.grid(axis='y', alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Temporal analysis\n",
    "\n",
    "Examine how AI framing has evolved over parliamentary years."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# aggregate by year and document type\ntemporal = df.groupby(['year', 'doc_type']).agg({\n    'opp_per_1k': 'mean',\n    'risk_per_1k': 'mean',\n    'framing_ratio': 'mean',\n    'doc_id': 'count'\n}).rename(columns={'doc_id': 'n_docs'}).reset_index()\n\n# filter years with sufficient documents\ntemporal = temporal[temporal['n_docs'] >= 5]\n\n# create sortable year column (extract first year as integer)\ndef year_to_int(year_str):\n    \"\"\"convert '2018/19' to 2018 for sorting.\"\"\"\n    try:\n        return int(year_str.split('/')[0])\n    except:\n        return 0\n\ntemporal['year_sort'] = temporal['year'].apply(year_to_int)\ntemporal = temporal.sort_values('year_sort')\n\n# focus on recent years where both document types have substantial data (2017+)\ntemporal_recent = temporal[temporal['year_sort'] >= 2017]\n\nprint(f\"years with data (n>=5): {temporal['year'].nunique()}\")\nprint(f\"recent years (2017+): {temporal_recent['year'].nunique()}\")\ntemporal_recent"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# visualisation: framing over time (recent years with both doc types)\nfig, axes = plt.subplots(2, 1, figsize=(12, 8))\n\n# get unique years in order for x-axis\nyears_order = temporal_recent.sort_values('year_sort')['year'].unique()\n\n# opportunity vs risk trends\nfor doc_type, color in [('mot', '#9b59b6'), ('prop', '#e91e63')]:\n    data = temporal_recent[temporal_recent['doc_type'] == doc_type].copy()\n    data = data.sort_values('year_sort')\n    label = 'motioner' if doc_type == 'mot' else 'propositioner'\n    \n    # use year_sort for x position to ensure proper ordering\n    axes[0].plot(data['year_sort'], data['opp_per_1k'], \n                marker='o', linewidth=2, color=color, alpha=0.8,\n                label=f'{label} (opportunity)')\n    axes[0].plot(data['year_sort'], data['risk_per_1k'], \n                marker='s', linewidth=2, color=color, alpha=0.4, linestyle='--',\n                label=f'{label} (risk)')\n\naxes[0].set_ylabel('mean frequency (per 1000 words)')\naxes[0].set_title('opportunity and risk framing over time (2017+)')\naxes[0].legend(loc='upper left')\naxes[0].grid(alpha=0.3)\naxes[0].set_xticks([int(y.split('/')[0]) for y in years_order])\naxes[0].set_xticklabels(years_order, rotation=45, ha='right')\n\n# framing ratio trends\nfor doc_type, color in [('mot', '#9b59b6'), ('prop', '#e91e63')]:\n    data = temporal_recent[temporal_recent['doc_type'] == doc_type].copy()\n    data = data.sort_values('year_sort')\n    label = 'motioner' if doc_type == 'mot' else 'propositioner'\n    \n    axes[1].plot(data['year_sort'], data['framing_ratio'], \n                marker='o', linewidth=2, color=color, label=label)\n\naxes[1].axhline(0, color='#95a5a6', linestyle='--', linewidth=1, alpha=0.7)\naxes[1].set_xlabel('parliamentary year')\naxes[1].set_ylabel('mean framing ratio')\naxes[1].set_title('net framing (opportunity - risk) over time')\naxes[1].legend()\naxes[1].grid(alpha=0.3)\naxes[1].set_xticks([int(y.split('/')[0]) for y in years_order])\naxes[1].set_xticklabels(years_order, rotation=45, ha='right')\n\nplt.tight_layout()\nplt.show()"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary statistics and export\n",
    "\n",
    "Generate final summary statistics and export results for further analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# overall framing summary\n",
    "print(\"=\" * 60)\n",
    "print(\"FRAMING ANALYSIS SUMMARY\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"\\ntotal documents analysed: {len(df)}\")\n",
    "print(f\"  - motioner: {len(df[df['doc_type'] == 'mot'])}\")\n",
    "print(f\"  - propositioner: {len(df[df['doc_type'] == 'prop'])}\")\n",
    "\n",
    "print(\"\\nopportunity framing (per 1000 words):\")\n",
    "for doc_type in ['mot', 'prop']:\n",
    "    mean_val = df[df['doc_type'] == doc_type]['opp_per_1k'].mean()\n",
    "    label = 'motioner' if doc_type == 'mot' else 'propositioner'\n",
    "    print(f\"  {label}: {mean_val:.2f}\")\n",
    "\n",
    "print(\"\\nrisk framing (per 1000 words):\")\n",
    "for doc_type in ['mot', 'prop']:\n",
    "    mean_val = df[df['doc_type'] == doc_type]['risk_per_1k'].mean()\n",
    "    label = 'motioner' if doc_type == 'mot' else 'propositioner'\n",
    "    print(f\"  {label}: {mean_val:.2f}\")\n",
    "\n",
    "print(\"\\nnet framing ratio (opp - risk):\")\n",
    "for doc_type in ['mot', 'prop']:\n",
    "    mean_val = df[df['doc_type'] == doc_type]['framing_ratio'].mean()\n",
    "    label = 'motioner' if doc_type == 'mot' else 'propositioner'\n",
    "    print(f\"  {label}: {mean_val:.2f}\")\n",
    "\n",
    "print(\"\\ndocuments with ai mentions: {}\".format(len(df_with_ai)))\n",
    "print(\"\\naverage context sentiment (vader compound):\")\n",
    "for doc_type in ['mot', 'prop']:\n",
    "    mean_val = df_with_ai[df_with_ai['doc_type'] == doc_type]['sentiment_compound'].mean()\n",
    "    label = 'motioner' if doc_type == 'mot' else 'propositioner'\n",
    "    print(f\"  {label}: {mean_val:.3f}\")\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export results\n",
    "output_dir = Path('/Users/simon/Dropbox/wrk/active/research/riksdagen-data/swe-rd-ai/results')\n",
    "output_dir.mkdir(exist_ok=True)\n",
    "\n",
    "# save document-level results\n",
    "export_cols = ['doc_id', 'doc_type', 'title', 'date', 'year', 'word_count',\n",
    "               'opp_count', 'risk_count', 'opp_per_1k', 'risk_per_1k', \n",
    "               'framing_ratio', 'ai_mention_count', 'sentiment_compound']\n",
    "df[export_cols].to_csv(output_dir / 'rq1_document_framing.csv', index=False)\n",
    "\n",
    "# save temporal aggregates\n",
    "temporal.to_csv(output_dir / 'rq1_temporal_framing.csv', index=False)\n",
    "\n",
    "print(f\"results exported to {output_dir}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}